{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YTHbhzIo8M51"
      },
      "source": [
        "## Step 0: Install and Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a21WMXtH8Vlr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchinfo import summary"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "L5RmV_wi7syw"
      },
      "source": [
        "# Step 1: Get the Data\n",
        "\n",
        "1.1. Download the Data from the Web (It will be a .zip file for this)\n",
        "\n",
        "1.2. Extract the zip file\n",
        "\n",
        "1.3. Delete the zip file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-lIMXd58MPD",
        "outputId": "08d76bdd-ce1d-41e9-ac3d-b3193d5dff6f"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# Define the URL for the zip file\n",
        "url = \"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\"\n",
        "\n",
        "# Send a GET request to download the file\n",
        "response = requests.get(url)\n",
        "\n",
        "# Define the path to the data directory\n",
        "data_path = Path(\"data\")\n",
        "\n",
        "# Define the path to the image directory\n",
        "image_path = data_path / \"pizza_steak_sushi\"\n",
        "\n",
        "# Check if the image directory already exists\n",
        "if image_path.is_dir():\n",
        "    print(f\"{image_path} directory exists.\")\n",
        "else:\n",
        "    print(f\"Did not find {image_path} directory, creating one...\")\n",
        "    image_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Write the downloaded content to a zip file\n",
        "with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "# Extract the contents of the zip file to the image directory\n",
        "with ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zipref:\n",
        "    zipref.extractall(image_path)\n",
        "\n",
        "# Remove the downloaded zip file\n",
        "os.remove(data_path / \"pizza_steak_sushi.zip\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lZGJyGxA8a38"
      },
      "source": [
        "# Step 2: Define Transformations\n",
        "\n",
        "1. Resize the images using `Resize()` to 224. We choose the images size to be 224 based on the ViT Paper\n",
        "\n",
        "2. Convert to Tensor using `ToTensor()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jz-wGAha8hgk"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import Resize, Compose, ToTensor\n",
        "\n",
        "# Define the train_transform using Compose\n",
        "train_transform = Compose([Resize((224, 224)), ToTensor()])\n",
        "\n",
        "# Define the test_transform using Compose\n",
        "test_transform = Compose([Resize((224, 224)), ToTensor()])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QY8ySv3r7zFM"
      },
      "source": [
        "# Step 3: Create Dataset and DataLoader\n",
        "\n",
        "We can use PyTorch's ImageFolder DataSet library to create our Datasets.\n",
        "\n",
        "For ImageFolder to work this is how your data folder needs to be structured.\n",
        "\n",
        "```markdown\n",
        "data\n",
        "└── pizza_steak_sushi\n",
        "    ├── test\n",
        "    │   ├── pizza\n",
        "    │   ├── steak\n",
        "    │   └── sushi\n",
        "    └── train\n",
        "        ├── pizza\n",
        "        ├── steak\n",
        "        └── sushi\n",
        "```\n",
        "All the `pizza` images will be in the pizza folder of train and test sub folders and so on for all the classes that you have.\n",
        "\n",
        "There are two useful methods that you can call on the created `training_dataset` and `test_dataset`\n",
        "\n",
        "1. `training_dataset.classes` that gives `['pizza', 'steak', 'sushi']`\n",
        "2. `training_dataset.class_to_idx` that gives `{'pizza': 0, 'steak': 1, 'sushi': 2}`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNQrilEG8LH2"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "# Define the data directory\n",
        "data_dir = Path(\"data/pizza_steak_sushi\")\n",
        "\n",
        "# Create the training dataset using ImageFolder\n",
        "training_dataset = ImageFolder(root=data_dir / \"train\", transform=train_transform)\n",
        "\n",
        "# Create the test dataset using ImageFolder\n",
        "test_dataset = ImageFolder(root=data_dir / \"test\", transform=test_transform)\n",
        "\n",
        "# Create the training dataloader using DataLoader\n",
        "training_dataloader = DataLoader(\n",
        "    dataset=training_dataset, shuffle=True, batch_size=BATCH_SIZE, num_workers=2\n",
        ")\n",
        "\n",
        "# Create the test dataloader using DataLoader\n",
        "test_dataloader = DataLoader(\n",
        "    dataset=test_dataset, shuffle=False, batch_size=BATCH_SIZE, num_workers=2\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LpK4bP1WNyYV"
      },
      "source": [
        "We can visualize a few training dataset images and see their labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 907
        },
        "id": "DASH8mQXNc5m",
        "outputId": "e01996a6-2a2a-4864-fa85-e4529ba28130"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "num_rows = 5\n",
        "num_cols = num_rows\n",
        "\n",
        "# Create a figure with subplots\n",
        "fig, axs = plt.subplots(num_rows, num_cols, figsize=(10, 10))\n",
        "\n",
        "# Iterate over the subplots and display random images from the training dataset\n",
        "for i in range(num_rows):\n",
        "    for j in range(num_cols):\n",
        "        # Choose a random index from the training dataset\n",
        "        image_index = random.randrange(len(training_dataset))\n",
        "\n",
        "        # Display the image in the subplot\n",
        "        axs[i, j].imshow(training_dataset[image_index][0].permute((1, 2, 0)))\n",
        "\n",
        "        # Set the title of the subplot as the corresponding class name\n",
        "        axs[i, j].set_title(\n",
        "            training_dataset.classes[training_dataset[image_index][1]], color=\"white\"\n",
        "        )\n",
        "\n",
        "        # Disable the axis for better visualization\n",
        "        axs[i, j].axis(False)\n",
        "\n",
        "# Set the super title of the figure\n",
        "fig.suptitle(\n",
        "    f\"Random {num_rows * num_cols} images from the training dataset\",\n",
        "    fontsize=16,\n",
        "    color=\"white\",\n",
        ")\n",
        "\n",
        "# Set the background color of the figure as black\n",
        "fig.set_facecolor(color=\"black\")\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PUAsSxKVkumd"
      },
      "source": [
        "# Understanding Vision Transformer\n",
        "\n",
        "Let us take some time now to understand the Vision Transformer Architecture. This is the link to the original vision transformer paper: https://arxiv.org/abs/2010.11929.\n",
        "\n",
        "Below you can see the architecture that is proposed in the image.\n",
        "\n",
        "<img height = \"430\" src = \"https://res.cloudinary.com/dltwftrgc/image/upload/v1686823579/Blogs/Vision-Transformer/Vision_Transformer_Architecture_woi9aw.png\">\n",
        "\n",
        "The Vision Transformer (ViT) is a type of Transformer architecture designed for image processing tasks. Unlike traditional Transformers that operate on sequences of word embeddings, ViT operates on sequences of image embeddings. In other words, it breaks down an input image into patches and treats them as a sequence of learnable embeddings.\n",
        "\n",
        "At a broad level, what ViT does is, it:\n",
        "\n",
        "1. **Creates Patch Emebeddings**\n",
        "     1. Takes an input image of a given size $(H \\times W \\times C)$ -> (Height, Width, Channels)\n",
        "     2. Breaks it down into $N$ patches of a given size: $P$ --> PATCH_SIZE\n",
        "     3. Converts the patches into a **sequence** of learnable embeddings vectors: $E ∈ R^{(P^2C) \\times D}$\n",
        "     4. Prepends a **classification token embedding vector** to the learnable embeddings vectors\n",
        "     5. Adds the **position embeddings** to the learnable embeddings: $E_{pos} \\in R^{(N+1) \\times D}$\n",
        "   \n",
        "2. Passes embeddings through **Transformer Blocks**:\n",
        "\n",
        "     - The patch embeddings, along with the classification token, are passed through multiple Transformer blocks.\n",
        "     - Each Transformer block consists of a MultiHead Self-Attention Block (MSA Block) and a Multi-Layer Perceptron Block (MLP Block).\n",
        "     - Skip connections are established between the input to the Transformer block and the input to the MSA block, as well as between the input to the MLP block and the output of the MLP block. These skip connections help mitigate the vanishing gradient problem as more Transformer blocks are added.\n",
        "  \n",
        "3. Performs **Classification**:\n",
        "\n",
        "     - The final output from the Transformer blocks is passed through an MLP block.\n",
        "     - The classification token, which contains information about the input image's class, is used to make predictions.\n",
        "\n",
        "\n",
        "We will dive into each of these steps in detail, starting with the crucial process of creating patch embeddings.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "adaB-K-MPyB4"
      },
      "source": [
        "# Step 4: Create Patch Embedding Layer\n",
        "\n",
        "For the ViT paper we need to perform the following functions on the image before passing to the MultiHead Self Attention Transformer Layer\n",
        "\n",
        "1. Convert the image into patches of 16 x 16 size.\n",
        "2. Embed each patch into 768 dimensions. So each patch becomes a `[1 x 768] ` Vector. There will be $N = \\frac{H \\times W}{P^2}$ number of patches for each image. This results in an image that is of the shape `[14 x 14 x 768]`\n",
        "3.  Flatten the image along a single vector. This will give a `[196 x 768]` Matrix which is our Image Embedding Sequence.\n",
        "4. Prepend the Class Token Embeddings to the above output\n",
        "5. Add the Position Embeddings to the Class Token and Image Embeddings.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "![Vision Tranformer Steps](https://github.com/mrdbourke/pytorch-deep-learning/raw/main/images/08-vit-paper-patch-embedding-animation.gif)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tE_WI89TUEv"
      },
      "outputs": [],
      "source": [
        "PATCH_SIZE = 16\n",
        "IMAGE_WIDTH = 224\n",
        "IMAGE_HEIGHT = IMAGE_WIDTH\n",
        "IMAGE_CHANNELS = 3\n",
        "EMBEDDING_DIMS = IMAGE_CHANNELS * PATCH_SIZE**2\n",
        "NUM_OF_PATCHES = int((IMAGE_WIDTH * IMAGE_HEIGHT) / PATCH_SIZE**2)\n",
        "\n",
        "# the image width and image height should be divisible by patch size. This is a check to see that.\n",
        "\n",
        "assert IMAGE_WIDTH % PATCH_SIZE == 0 and IMAGE_HEIGHT % PATCH_SIZE == 0, print(\n",
        "    \"Image Width is not divisible by patch size\"\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YpaPIhlnUE76"
      },
      "source": [
        "## Step 4.1 Converting the image into patches of 16 x 16 and creating an embedding vector for each patch of size 768.\n",
        "\n",
        "This can be accomplished by using a Conv2D Layer with a kernel_size equal to patch_size and a stride equal to patch_size\n",
        "\n",
        "<img src = \"https://res.cloudinary.com/dltwftrgc/image/upload/v1686832476/Blogs/Vision-Transformer/CNN_Flatten_vfq1q6.png\" width = \"600\">\n",
        "<!-- ![Convolution plus Flatten](https://res.cloudinary.com/dltwftrgc/image/upload/v1686832476/Blogs/Vision-Transformer/CNN_Flatten_vfq1q6.png) -->\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-x1NsH3_XX0I"
      },
      "outputs": [],
      "source": [
        "conv_layer = nn.Conv2d(in_channels=IMAGE_CHANNELS,\n",
        "                       out_channels=EMBEDDING_DIMS, kernel_size=PATCH_SIZE, stride=PATCH_SIZE)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "J7tWMC4BX207"
      },
      "source": [
        "We can pass a random image into the convolutional layer and see what happens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "JuCx8owJXqm2",
        "outputId": "dba4904e-80a4-4b27-843e-e7efd6faec14"
      },
      "outputs": [],
      "source": [
        "random_images, random_labels = next(iter(training_dataloader))\n",
        "random_image = random_images[0]\n",
        "\n",
        "# Create a new figure\n",
        "fig = plt.figure(1)\n",
        "\n",
        "# Display the random image\n",
        "plt.imshow(random_image.permute((1, 2, 0)))\n",
        "\n",
        "# Disable the axis for better visualization\n",
        "plt.axis(False)\n",
        "\n",
        "# Set the title of the image\n",
        "plt.title(training_dataset.classes[random_labels[0]], color=\"white\")\n",
        "\n",
        "# Set the background color of the figure as black\n",
        "fig.set_facecolor(color=\"black\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JuPtguaoZlJr"
      },
      "source": [
        "We need to change the shape to `[1, 14, 14, 768]` and flatten the output to `[1, 196, 768]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siVK573vZD44",
        "outputId": "2abac1ef-b655-4857-e44b-4dca2b98cc8c"
      },
      "outputs": [],
      "source": [
        "# Pass the image through the convolution layer\n",
        "image_through_conv = conv_layer(random_image.unsqueeze(0))\n",
        "print(\n",
        "    f\"Shape of embeddings through the conv layer -> {list(image_through_conv.shape)} <- [batch_size, num_of_patch_rows,num_patch_cols embedding_dims]\"\n",
        ")\n",
        "\n",
        "# Permute the dimensions of image_through_conv to match the expected shape\n",
        "image_through_conv = image_through_conv.permute((0, 2, 3, 1))\n",
        "\n",
        "# Create a flatten layer using nn.Flatten\n",
        "flatten_layer = nn.Flatten(start_dim=1, end_dim=2)\n",
        "\n",
        "# Pass the image_through_conv through the flatten layer\n",
        "image_through_conv_and_flatten = flatten_layer(image_through_conv)\n",
        "\n",
        "# Print the shape of the embedded image\n",
        "print(\n",
        "    f\"Shape of embeddings through the flatten layer -> {list(image_through_conv_and_flatten.shape)} <- [batch_size, num_of_patches, embedding_dims]\"\n",
        ")\n",
        "\n",
        "# Assign the embedded image to a variable\n",
        "embedded_image = image_through_conv_and_flatten"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lRdGGTLq-pkm"
      },
      "source": [
        "## 4.2. Prepending the Class Token Embedding and Adding the Position Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jex6cn6moLSC",
        "outputId": "a386fad5-43c5-4dcc-9b67-1385d8d7470f"
      },
      "outputs": [],
      "source": [
        "class_token_embeddings = nn.Parameter(\n",
        "    torch.rand((1, 1, EMBEDDING_DIMS), requires_grad=True)\n",
        ")\n",
        "print(\n",
        "    f\"Shape of class_token_embeddings --> {list(class_token_embeddings.shape)} <-- [batch_size, 1, emdedding_dims]\"\n",
        ")\n",
        "\n",
        "embedded_image_with_class_token_embeddings = torch.cat(\n",
        "    (class_token_embeddings, embedded_image), dim=1\n",
        ")\n",
        "print(\n",
        "    f\"\\nShape of image embeddings with class_token_embeddings --> {list(embedded_image_with_class_token_embeddings.shape)} <-- [batch_size, num_of_patches+1, embeddiing_dims]\"\n",
        ")\n",
        "\n",
        "position_embeddings = nn.Parameter(\n",
        "    torch.rand((1, NUM_OF_PATCHES + 1, EMBEDDING_DIMS), requires_grad=True)\n",
        ")\n",
        "print(\n",
        "    f\"\\nShape of position_embeddings --> {list(position_embeddings.shape)} <-- [batch_size, num_patches+1, embeddings_dims]\"\n",
        ")\n",
        "\n",
        "final_embeddings = embedded_image_with_class_token_embeddings + position_embeddings\n",
        "print(\n",
        "    f\"\\nShape of final_embeddings --> {list(final_embeddings.shape)} <-- [batch_size, num_patches+1, embeddings_dims]\"\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_3QH341Bznf_"
      },
      "source": [
        "Shape of `class_token_embeddings` --> `[1, 1, 768]` <-- `[batch_size, 1, emdedding_dims]`\n",
        "\n",
        "Shape of `image embeddings with class_token_embeddings` --> `[1, 197, 768]` <-- `[batch_size, num_of_patches+1, embeddiing_dims]`\n",
        "\n",
        "Shape of `position_embeddings` --> `[1, 197, 768]` <-- `[batch_size, num_patches+1, embeddings_dims]`\n",
        "\n",
        "Shape of `final_embeddings` --> `[1, 197, 768]` <-- `[batch_size, num_patches+1, embeddings_dims]`"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "U_vz0mXkcBiH"
      },
      "source": [
        "## Put the PatchEmbedddingLayer Together\n",
        "\n",
        "We will inherit from the PyTorch `nn.Module` to create our custom layer which takes in an image and throws out the patch embeddings which consists of the Image Embeddings, Class Token Embeddings and the Position Embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HceUpSphcM8F"
      },
      "outputs": [],
      "source": [
        "class PatchEmbeddingLayer(nn.Module):\n",
        "    def __init__(self, in_channels, patch_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.in_channels = in_channels\n",
        "        self.conv_layer = nn.Conv2d(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=embedding_dim,\n",
        "            kernel_size=patch_size,\n",
        "            stride=patch_size,\n",
        "        )\n",
        "        self.flatten_layer = nn.Flatten(start_dim=1, end_dim=2)\n",
        "        self.class_token_embeddings = nn.Parameter(\n",
        "            torch.rand((BATCH_SIZE, 1, EMBEDDING_DIMS), requires_grad=True)\n",
        "        )\n",
        "        self.position_embeddings = nn.Parameter(\n",
        "            torch.rand((1, NUM_OF_PATCHES + 1, EMBEDDING_DIMS), requires_grad=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = (\n",
        "            torch.cat(\n",
        "                (\n",
        "                    self.class_token_embeddings,\n",
        "                    self.flatten_layer(self.conv_layer(x).permute((0, 2, 3, 1))),\n",
        "                ),\n",
        "                dim=1,\n",
        "            )\n",
        "            + self.position_embeddings\n",
        "        )\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzVMPWNFcdX0",
        "outputId": "c48c21ad-1c1b-44a1-96e9-a0a004ef46e2"
      },
      "outputs": [],
      "source": [
        "patch_embedding_layer = PatchEmbeddingLayer(\n",
        "    in_channels=IMAGE_CHANNELS,\n",
        "    patch_size=PATCH_SIZE,\n",
        "    embedding_dim=IMAGE_CHANNELS * PATCH_SIZE**2,\n",
        ")\n",
        "\n",
        "patch_embeddings = patch_embedding_layer(random_images)\n",
        "patch_embeddings.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "summary(model=patch_embedding_layer,\n",
        "        input_size=(BATCH_SIZE, 3, 224, 224), # (batch_size, input_channels, img_width, img_height)\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiHeadSelfAttentionBlock(nn.Module):\n",
        "  def __init__(self,\n",
        "               embedding_dims = 768, # Hidden Size D in the ViT Paper Table 1\n",
        "               num_heads = 12,  # Heads in the ViT Paper Table 1\n",
        "               attn_dropout = 0.0 # Default to Zero as there is no dropout for the the MSA Block as per the ViT Paper\n",
        "               ):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embedding_dims = embedding_dims\n",
        "    self.num_head = num_heads\n",
        "    self.attn_dropout = attn_dropout\n",
        "\n",
        "    self.layernorm = nn.LayerNorm(normalized_shape = embedding_dims)\n",
        "\n",
        "    self.multiheadattention =  nn.MultiheadAttention(num_heads = num_heads,\n",
        "                                                     embed_dim = embedding_dims,\n",
        "                                                     dropout = attn_dropout,\n",
        "                                                     batch_first = True,\n",
        "                                                    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.layernorm(x)\n",
        "    output,_ = self.multiheadattention(query=x, key=x, value=x,need_weights=False)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "multihead_self_attention_block = MultiHeadSelfAttentionBlock(embedding_dims = EMBEDDING_DIMS,\n",
        "                                                             num_heads = 12\n",
        "                                                             )\n",
        "print(f'Shape of the input Patch Embeddings => {list(patch_embeddings.shape)} <= [batch_size, num_patches+1, embedding_dims ]')\n",
        "print(f'Shape of the output from MSA Block => {list(multihead_self_attention_block(patch_embeddings).shape)} <= [batch_size, num_patches+1, embedding_dims ]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "summary(model=multihead_self_attention_block,\n",
        "        input_size=(1, 197, 768), # (batch_size, num_patches, embedding_dimension)\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MachineLearningPerceptronBlock(nn.Module):\n",
        "  def __init__(self, embedding_dims, mlp_size, mlp_dropout):\n",
        "    super().__init__()\n",
        "    self.embedding_dims = embedding_dims\n",
        "    self.mlp_size = mlp_size\n",
        "    self.dropout = mlp_dropout\n",
        "\n",
        "    self.layernorm = nn.LayerNorm(normalized_shape = embedding_dims)\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(in_features = embedding_dims, out_features = mlp_size),\n",
        "        nn.GELU(),\n",
        "        nn.Dropout(p = mlp_dropout),\n",
        "        nn.Linear(in_features = mlp_size, out_features = embedding_dims),\n",
        "        nn.Dropout(p = mlp_dropout)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.mlp(self.layernorm(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mlp_block = MachineLearningPerceptronBlock(embedding_dims = EMBEDDING_DIMS,\n",
        "                                           mlp_size = 3072,\n",
        "                                           mlp_dropout = 0.1)\n",
        "\n",
        "summary(model=mlp_block,\n",
        "        input_size=(1, 197, 768), # (batch_size, num_patches, embedding_dimension)\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, embedding_dims = 768,\n",
        "               mlp_dropout=0.1,\n",
        "               attn_dropout=0.0,\n",
        "               mlp_size = 3072,\n",
        "               num_heads = 12,\n",
        "               ):\n",
        "    super().__init__()\n",
        "\n",
        "    self.msa_block = MultiHeadSelfAttentionBlock(embedding_dims = embedding_dims,\n",
        "                                                 num_heads = num_heads,\n",
        "                                                 attn_dropout = attn_dropout)\n",
        "\n",
        "    self.mlp_block = MachineLearningPerceptronBlock(embedding_dims = embedding_dims,\n",
        "                                                    mlp_size = mlp_size,\n",
        "                                                    mlp_dropout = mlp_dropout,\n",
        "                                                    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.msa_block(x) + x\n",
        "    x = self.mlp_block(x) + x\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transformer_block = TransformerBlock(embedding_dims = EMBEDDING_DIMS,\n",
        "                                     mlp_dropout = 0.1,\n",
        "                                     attn_dropout=0.0,\n",
        "                                     mlp_size = 3072,\n",
        "                                     num_heads = 12)\n",
        "\n",
        "print(f'Shape of the input Patch Embeddings => {list(patch_embeddings.shape)} <= [batch_size, num_patches+1, embedding_dims ]')\n",
        "print(f'Shape of the output from Transformer Block => {list(transformer_block(patch_embeddings).shape)} <= [batch_size, num_patches+1, embedding_dims ]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "summary(model=transformer_block,\n",
        "        input_size=(1, 197, 768), # (batch_size, num_patches, embedding_dimension)\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ViT(nn.Module):\n",
        "  def __init__(self, img_size = 224,\n",
        "               in_channels = 3,\n",
        "               patch_size = 16,\n",
        "               embedding_dims = 768,\n",
        "               num_transformer_layers = 12, # from table 1 above\n",
        "               mlp_dropout = 0.1,\n",
        "               attn_dropout = 0.0,\n",
        "               mlp_size = 3072,\n",
        "               num_heads = 12,\n",
        "               num_classes = 1000):\n",
        "    super().__init__()\n",
        "\n",
        "    self.patch_embedding_layer = PatchEmbeddingLayer(in_channels = in_channels,\n",
        "                                                     patch_size=patch_size,\n",
        "                                                     embedding_dim = embedding_dims)\n",
        "\n",
        "    self.transformer_encoder = nn.Sequential(*[TransformerBlock(embedding_dims = embedding_dims,\n",
        "                                              mlp_dropout = mlp_dropout,\n",
        "                                              attn_dropout = attn_dropout,\n",
        "                                              mlp_size = mlp_size,\n",
        "                                              num_heads = num_heads) for _ in range(num_transformer_layers)])\n",
        "\n",
        "    self.classifier = nn.Sequential(nn.LayerNorm(normalized_shape = embedding_dims),\n",
        "                                    nn.Linear(in_features = embedding_dims,\n",
        "                                              out_features = num_classes))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.classifier(self.transformer_encoder(self.patch_embedding_layer(x))[:, 0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vit = ViT()\n",
        "summary(model=vit,\n",
        "        input_size=(BATCH_SIZE, 3, 224, 224), # (batch_size, num_patches, embedding_dimension)\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm,trange\n",
        "from torch.optim import Adam\n",
        "device =  torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device: \", device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "N_EPOCHS = 5\n",
        "LR = 0.005\n",
        "model = ViT().to(device)\n",
        "optimizer = Adam(model.parameters(), lr=LR)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "for epoch in trange(N_EPOCHS, desc=\"Training\"):\n",
        "    train_loss = 0.0\n",
        "    for batch in tqdm(training_dataloader, desc=f\"Epoch {epoch + 1} in training\", leave=False):\n",
        "        x, y = batch\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        y_hat = model(x)\n",
        "\n",
        "        loss = criterion(y_hat, y)\n",
        "\n",
        "        train_loss += loss.detach().cpu().item() / len(training_dataloader)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{N_EPOCHS} loss: {train_loss:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
